from pyspark.sql import SparkSession
from pyspark.sql.types import StructType

""" 
                                ğŸ†‚ğŸ†„ğŸ…¿ğŸ…´ğŸ†ğŸ…»ğŸ…¾ğŸ…¶ğŸ…¸ğŸ…²ğŸ…°
DocumentaÃ§Ã£o:
O     motivo da     existÃªncia desse   arquivo     Ã©    uma       resposta a    questÃ£o 4        do nosso case
infelizmente    nÃ£o  tive tempo  hÃ¡bil    para   escrever   toda  a pipeline     usando  Structured Streamings
porÃ©m   dei o  ponta pÃ© inicial,  como pode   ver  esse    script consegue  a partir de  um  evento de criaÃ§Ã£o
de   um arquivo  numa pasta,  nesse  caso  na  pasta   de  landing de  condominios,    startar o processamento
em    streaming usando  micro baching   ele  nÃ£o   Ã©   uma   resposta  completa ao  desafio,    mas como disse
anteriormente,   Ã©   um ponto de  partida.   Para    aumento    de  escabilidade vertical    pode ser definido
um     maior nÃºmero  de works a  partir    do  mÃ©todo    config de   Spark Session     e   tambÃ©m mais memÃ³ria
RAM  para processamento,  jÃ¡ numa  estratÃ©gia  de  escala   horizontal Ã© possÃ­vel  executar  esse mesmo script
em  infra serverless em diversos clusters porÃ©m Ã© necessÃ¡rio analisar  questÃµes de  duplicicade e concorrÃªncia
de recursos (recomendo o uso de um  serviÃ§o de mensageria como kafka para uma estratÃ©gia de escala horizontal).
"""

"""
Inicializa variÃ¡veis que serÃ£o utilizadas por todo o fluxo
"""
def start_up():
        
    global spark
    spark = SparkSession.builder.appName("streaming_raw_data").getOrCreate()
    
    global schema_townhouse, schema_property, schema_residents, schema_transactions
    schema_townhouse = StructType().add("condominio_id", "string").add("condominio_nome", "string").add("condominio_endereco", "string")
  
    global input_path_townhouse
    input_path_townhouse = "./datalake/landing/dim_condominios"

"""
Inicia streaming de dados.
"""   
def work_streaming_townhouse():
    
    inputDF = spark.readStream.format("csv").schema(schema_townhouse).format("csv").option("maxFilesPerTrigger", 10).option("header","true").csv(input_path_townhouse)
    
    transformedDF = inputDF
    
    query = transformedDF.writeStream.outputMode("append").format("parquet").option("checkpointLocation", "./datalake/raw/dim_condominios/_checkpoint").start("./datalake/raw/dim_condominios")
    
    query.awaitTermination()

"""
OrquestraÃ§Ã£o do script de ingestÃ£o.
"""
def main():
    
    start_up()
    
    work_streaming_townhouse()
    
    
if __name__=='__main__':
    main()