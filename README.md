üÜÇüÜÑüÖøüÖ¥üÜÅüÖªüÖæüÖ∂üÖ∏üÖ≤üÖ∞

Pipeline de dados do case de projeto para a empresa CondoManage

Aten√ß√£o: Respostas das quest√µes de encontram abaixo do manual de instala√ß√£o desse mesmo arquivo!

Para a correta execu√ß√£o desse projeto devemos nos atentar a instala√ß√£o de depend√™ncias:

OBS! Todo o projeto foi criado usando sitema operacional MAC a depender do seu sistema operacional
diferentes passos devem ser tomados para a devida instala√ß√£o do projeto, aqui estarei disponibilizando
apenas o passo a passo para o MAC e sistemas similares baseados em unix exceto pelo gerenciador de pacotes
que estou utilizando o brew

1. Garantir a instala√ß√£o do java

   brew install openjdk@11

2. Garantir instala√ß√£o do spark

   brew install apache-spark

3. Instalar python

   brew install python3

   Neste projeto foi usada a vers√£o Python 3.10.11 usar essa ou maior

4. Instalar o virtualenv na m√°quina local

   pip install virtualenv

5. Abrir projeto e iniciar o ambiente virtual

   source env/bin/activate

6. Instalar requirements

   pip install -r requirements.txt

7. Definir vari√°vel de ambiente do airflow

   export AIRFLOW_HOME=~/Desktop/case_super_logica

8. Para executar o projeto em modo local sem airflow via terminal

   cd dags;python3 superlogica_data_pipeline_local.py

   OBS! o motivo de rodar a partir da pasta de dags √© que no in√≠cio do projeto
   n√£o pensei que teria problemas em rodar a pipe com o airflow com scripts de
   execu√ß√£o paralela em modo local, por√©m ap√≥s consultar a documenta√ß√£o vi essa restri√ß√£o
   mas o projeto j√° estava todo montado a partir desse ponto de montagem

9. Para executar o projeto em modo local usando o airflow

   airflow standalone logue com admin e senha presente no arquivo
   standalone_admin_password

   OBS! voc√™ n√£o conseguir√° rodar scripts em paralelo como o presente em nossa DAG
   sem instalar um banco de dados local e alterar o airflow.cgf para sua configura√ß√£o
   de banco escolhida devido a esse trabalho recomendo fortemente o uso da execu√ß√£o via
   terminal presente no passo 8

-> RESPOSTAS

Quest√£o 1.

A arquitetura de datalake escolhida para a cria√ß√£o dessa projeto foi a medalh√£o
aqui temos:

camada landing apenas recebendo os arquivos csv para processamento

camada raw apenas transforma os arquivos da camada landing em formato parquet

camada refined aplica regras de neg√≥cio e gera a OBT (One Big Table) modalagem escolhida
por mim para este projeto

camada trusted √© uma c√≥pia da camada refined com dados prontos para an√°lise

Quest√£o 2.

A ingest√£o de dados desse projeto foi criada a partir de uma simula√ß√£o da cria√ß√£o de
arquivos csv a classe responsavel por isso √© a DataGenerator presente no arquivo
dags/models/datagenerator.py ela √© orquestrada a partir de dags/landing_read_data_from_external.py
e por sua vez os dados entram na camada raw atrav√©s de dags/raw_read_data_from_landing.py
a explica√ß√£o de como cada arquivo funciona est√° em seus coment√°rios internos.

Quest√£o 3.

Quest√£o 4.

Vide coment√°rios internos do arquivo dags/raw_streaming_read_data_from_landing_townhouse.py
l√° √© feita a primeira parte da ingest√£o usando streaming a pipelie exposta na DAG e no arquivo
dags/superlogica_data_pipeline_local.py √© do tipo batching

Quest√£o 5.

Para definic√£o de documenta√ß√£o usaria um reposit√≥rio de documenta√ß√£o como o confluence e tamb√©m
geraria coment√°rio seguindo a PEP8 e com a ferramenta mkdocs conseguiria fazer a documenta√ß√£o autom√°tica
a partir dos comet√°rios nos jobs.
