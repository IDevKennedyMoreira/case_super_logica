
![Logo](https://superlogica.design/static/dc2e4bc5690f7fe87eea0ba45af1374e/7f15f/superlogica-logo-color.png)



# CondoManage

Manual de intru√ß√µes de como instalar o projeto em m√°quina local




## Authors

- [@IDevKennedyMoreira](https://github.com/IDevKennedyMoreira)


## üîó Links
[![portfolio](https://img.shields.io/badge/my_portfolio-000?style=for-the-badge&logo=ko-fi&logoColor=white)](https://github.com/IDevKennedyMoreira/case_super_logica)
[![linkedin](https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](www.linkedin.com/in/kennedy-moreira-rocha-j√∫nior-3b3700128/)



## Local Deployment

Para deploy deste projeto em m√°quina local √© necess√°rio efetuar algumas ressalvas, este projeto foi desenvolvido em sistema operacional MacOs e os passos de instala√ß√£o podem mudar de acordo com sistema operacional, aqui basearei apenas a instala√ß√£o em MacOs pois n√£o possuo muita familiaridade com outros sistemas operacionais e n√£o saberia como descrever o passo a passo nesse momento.

Instalando o Java vers√£o 11.

```bash
  brew install openjdk@11
```

Instalando o Apache Spark.

```bash
  brew install apache-spark
```
Instalando o Python.

```bash
  brew install python3
```

Instalar o virtualenv na m√°quina local.

```bash
  pip install virtualenv
```
Abrir projeto e criar seu ambiente virtual.

```bash
  virtualenv venv
```
Abrir projeto e iniciar o ambiente virtual.

```bash
  source env/bin/activate
```
Instalar os requirements.

```bash
  pip install -r requirements.txt
```

Definir vari√°vel de ambiente do airflow.

```bash
   export AIRFLOW_HOME=~/Desktop/case_super_logica
```

Para executar o projeto em modo local sem airflow via terminal.

```bash
   cd dags;python3 superlogica_data_pipeline_local.py
```

   OBS! O motivo de rodar a partir da pasta de dags √© que no in√≠cio do projeto
   n√£o pensei que teria problemas em rodar a pipe com o airflow com scripts de
   execu√ß√£o paralela em modo local, por√©m ap√≥s consultar a documenta√ß√£o vi essa restri√ß√£o
   mas o projeto j√° estava todo montado a partir desse ponto de montagem.

Para executar o projeto em modo local usando o airflow
```bash
   airflow standalone 
```
   
   logue com usu√°rio admin e senha presente no arquivo
   standalone_admin_password

   OBS! Voc√™ n√£o conseguir√° rodar scripts em paralelo como o presente em nossa DAG
   sem instalar um banco de dados local e alterar o airflow.cgf para sua configura√ß√£o
   de banco escolhida. Devido a esse trabalho recomendo fortemente o uso da execu√ß√£o via
   terminal presente no passo anterior.
## Quest√£o 1

A arquitetura de datalake escolhida para a cria√ß√£o dessa projeto foi a medalh√£o
aqui temos:

Camada landing apenas recebendo os arquivos csv para processamento
camada raw apenas transforma os arquivos da camada landing em formato parquet;

camada refined aplica regras de neg√≥cio e gera a OBT (One Big Table) modalagem escolhida
por mim para este projeto;

camada trusted √© uma c√≥pia da camada refined com dados prontos para an√°lise.

## Quest√£o 2

A ingest√£o de dados desse projeto foi criada a partir de uma simula√ß√£o da cria√ß√£o de
arquivos csv a classe responsavel por isso √© a DataGenerator presente no arquivo
dags/models/datagenerator.py ela √© orquestrada a partir de dags/landing_read_data_from_external.py
e por sua vez os dados entram na camada raw atrav√©s de dags/raw_read_data_from_landing.py
a explica√ß√£o de como cada arquivo funciona est√° em seus coment√°rios internos.

## Quest√£o 3

## Quest√£o 4

Vide coment√°rios internos do arquivo dags/raw_streaming_read_data_from_landing_townhouse.py
l√° √© feita a primeira parte da ingest√£o usando streaming, por√©m pipeline exposta na DAG e no arquivo
dags/superlogica_data_pipeline_local.py √© do tipo batching.

## Quest√£o 5 

Para definic√£o de documenta√ß√£o usaria um reposit√≥rio de documenta√ß√£o como o confluence e tamb√©m
geraria coment√°rio seguindo a PEP8 e com a ferramenta mkdocs conseguiria fazer a documenta√ß√£o autom√°tica
a partir dos comet√°rios nos jobs.
Sobre as quest√µes envolvendo logging e monitoramento decidiria por usar a ELK stack tendo assim o Beatsfile
efetuando pooling em arquivos de dados e logs presentes na ferramenta de orquestra√ß√£o como o Airflow por sua vez o Beatsfile aciona o Logstash com a mensageria para que ele definir o melhor √≠ndice do ElasticSearch para inserir essa mensagem, por fim teriamos a liberdade de definir boards no Kibana com os logs propostos.
Para enriquecimento dos logs poderiamos criar uma classe de log com um decorator de logging onde este por sua vez persistiria a mensagem em arquivos de logs de cada job e tamb√©m colocaria o ELK stack para enxergar estes mesmos logs criando assim uma solu√ß√£o de monitoramento robusta.

![image](https://miro.medium.com/v2/resize:fit:1358/1*-JZ54LFogtjo8l-zB-DfEA.png)




## Badges

Licen√ßa:

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)


