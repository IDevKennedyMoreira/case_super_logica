
![Logo](https://superlogica.design/static/dc2e4bc5690f7fe87eea0ba45af1374e/7f15f/superlogica-logo-color.png)



# CondoManage

Manual de intru√ß√µes de como instalar o projeto em m√°quina local e resolu√ß√£o das quest√µes do case.




## Authors

- [@IDevKennedyMoreira](https://github.com/IDevKennedyMoreira)


## üîó Links
[![Project Repository](https://img.shields.io/badge/project_repository-006?&logo=github&style=for-the-badge&logoColor=white)](https://github.com/IDevKennedyMoreira/case_super_logica)

[![portfolio](https://img.shields.io/badge/my_portfolio-000?style=for-the-badge&logo=ko-fi&logoColor=white)](https://github.com/IDevKennedyMoreira/case_super_logica)

[![linkedin](https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](www.linkedin.com/in/kennedy-moreira-rocha-j√∫nior-3b3700128/)


## Local Deployment

Para deploy deste projeto em m√°quina local √© necess√°rio efetuar algumas ressalvas, este projeto foi desenvolvido em sistema operacional MacOs e os passos de instala√ß√£o podem mudar de acordo com sistema operacional, aqui basearei apenas a instala√ß√£o em MacOs pois n√£o possuo muita familiaridade com outros sistemas operacionais e n√£o saberia como descrever o passo a passo nesse momento pois n tenho acesso facilitado a outros sistemas operacionais.

Instalando o Java vers√£o 11.

```bash
  brew install openjdk@11
```

Instalando o Apache Spark.

```bash
  brew install apache-spark
```
Instalando o Python.

```bash
  brew install python3
```

Instalar o virtualenv na m√°quina local.

```bash
  pip install virtualenv
```
Abrir projeto e criar seu ambiente virtual.

```bash
  virtualenv venv
```
Abrir projeto e iniciar o ambiente virtual.

```bash
  source env/bin/activate
```
Instalar os requirements.

```bash
  pip install -r requirements.txt
```

Definir vari√°vel de ambiente do airflow.

```bash
   export AIRFLOW_HOME=~/Desktop/case_super_logica
```

Para executar o projeto em modo local sem airflow via terminal.

Para executar o projeto em modo local usando o airflow
```bash
   airflow standalone 
```
   
Acesse http://localhost:8080
logue com usu√°rio admin e senha presente no arquivo
standalone_admin_password e acione a DAG superlogica_data_pipeline

Para execu√ß√£o da pipeline streaming saia do Airflow e execute o arquivo dags/stream_read_data_from_landing.py. Em outro terminal
execute

```bash
   cd dags 
```

E para cria√ß√£o de novos arquivos na camada de landing execute

```bash
   python3 landing_read_data_from_external.py 
```



## Quest√£o 1

A arquitetura de datalake escolhida para a cria√ß√£o dessa projeto foi a medalh√£o
aqui temos:

Camada landing apenas recebendo os arquivos csv para processamento
camada raw apenas transforma os arquivos da camada landing em formato parquet;

camada refined aplica regras de neg√≥cio e gera a OBT (One Big Table) modalagem escolhida
por mim para este projeto;

camada trusted √© uma c√≥pia da camada refined com dados prontos para an√°lise.

## Quest√£o 2

A ingest√£o de dados desse projeto foi criada a partir de uma simula√ß√£o da cria√ß√£o de
arquivos csv a classe responsavel por isso √© a DataGenerator presente no arquivo
dags/models/datagenerator.py ela √© orquestrada a partir de dags/landing_read_data_from_external.py
e por sua vez os dados entram na camada raw atrav√©s de dags/raw_read_data_from_landing.py
a explica√ß√£o de como cada arquivo funciona est√° em seus coment√°rios internos.

## Quest√£o 3

A resolu√ß√£o dessa quest√£o est√° presente em dags/report_read_data_from_trusted.py

## Quest√£o 4

Vide coment√°rios internos do arquivo dags/raw_streaming_read_data_from_landing_townhouse.py
l√° √© feita a primeira parte da ingest√£o usando streaming, por√©m pipeline exposta na DAG e no arquivo
dags/superlogica_data_pipeline_local.py √© do tipo batching.

## Quest√£o 5 

Para definic√£o de documenta√ß√£o usaria um reposit√≥rio de documenta√ß√£o como o confluence e tamb√©m
geraria coment√°rio seguindo a PEP8 e com a ferramenta mkdocs conseguiria fazer a documenta√ß√£o autom√°tica
a partir dos comet√°rios nos jobs.
Sobre as quest√µes envolvendo logging e monitoramento decidiria por usar a ELK stack tendo assim o Beatsfile
efetuando pooling em arquivos de dados e logs presentes na ferramenta de orquestra√ß√£o como o Airflow por sua vez o Beatsfile aciona o Logstash com a mensageria para que ele definir o melhor √≠ndice do ElasticSearch para inserir essa mensagem, por fim teriamos a liberdade de definir boards no Kibana com os logs propostos.
Para enriquecimento dos logs poderiamos criar uma classe de log com um decorator de logging onde este por sua vez persistiria a mensagem em arquivos de logs de cada job e tamb√©m colocaria o ELK stack para enxergar estes mesmos logs criando assim uma solu√ß√£o de monitoramento robusta.

![image](https://miro.medium.com/v2/resize:fit:1358/1*-JZ54LFogtjo8l-zB-DfEA.png)

Para a arquitetura citada acima a presen√ßa de uma inst√¢ncia de Kafka entre o Beats e o Logstash pode sanar poss√≠veis gargalos de processamento.




## Badges

Licen√ßa:

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)


